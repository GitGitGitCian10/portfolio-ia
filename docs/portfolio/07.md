---
title: "UT2 - Backpropagation y Optimizadores"
date: 2025-01-01
---

# Backpropagation y Optimizadores

## Contexto
Esta actividad se centra en la experimentaci칩n de un MLP basada en
keras para que clasifique las im치genes del dataset CIFAR-10.

En la pr치ctica se experimentar치 con la arquitectura, optimizadores e hiperpar치metros
y funciones de callback para lograr aumentar lo m치ximo posible la capacidad del modelo
de clasificar correctamente las im치genes.

Adicionalmente, se utilizar치 TensorBoard para el an치lisis del entrenamiento del modelo
en sus diferentes etapas.

## Objetivos
- Desarrollar un MLP capaz de clasificar las im치genes de CIFAR-10
- Experimentar con diferentes arquitecturas
- Experimentar con diferentes optimizadores e hiperpar치metros
- Experimentar con diferentes callbacks

## Actividades
- Cargar datos
- Red neuronal

## Desarrollo

### Cargar datos
Para el desarrollo de la actividad, se utiliz칩 el dataset CIFAR-10, el cual consiste
en 60000 im치genes a color de 32x32 pixeles divididas equitativamente en 10 clases diferentes;
las clases siendo: "airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse", 
"ship" y "truck".

Estas im치genes fueron luego normalizadas a valores [-1, 1] para facilitar el entrenamiento,
se dividieron en sets de entrenamiento y validaci칩n, y se aplanaron las im치genes de
32x32x3 a un vector de tama침o 3072 para servir de input para el modelo MLP.

Esta es una visualizaci칩n de ejemplo de algunas im치genes del dataset junto con sus categor칤as.

![](../assets/07/01.png)

### Red neuronal
El modelo desarrollado result칩 en una red neuronal con 4 capas densas de 1024 neuronas con funci칩nes de 
activaci칩n de tipo relu y capas de dropout con 20% de probabilidad entre cada una; la
capa final presentando una funci칩n de activaci칩n de softmax para poder determinar las
clases.

Se implementaron diferentes optimizadores, eligiendo finalmente Adam por su mejor desempe침o
y un callback de EarlyStopping buscando evitar el overfitting del modelo.

El modelo fue entrenado en 100 epochs con tama침o de batch de 64, registrando las m칠tricas en
tensorboard, y finalmente evaluado tanto para el set de entrenamiento como para el de testeo.

```python linenums="1"
# === RED NEURONAL ===
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Crear modelo Sequential
model = keras.Sequential([
    layers.Dense(1024, activation='relu', input_shape=(x_train.shape[1],)),
    layers.Dropout(0.2),
    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(len(class_names), activation='softmax')  # salida binaria
])

optimizer = tf.keras.optimizers.Adam(
    learning_rate=0.0001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    loss_scale_factor=None,
    gradient_accumulation_steps=None,
    name='adam'
)


# Compilar modelo
model.compile(
    optimizer=optimizer,              # adam, sgd, rmsprop
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.01,
    patience=3,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=True,
    start_from_epoch=0,
)


# Entrenar
print("Entrenando red neuronal...")
run_dir = os.path.join(ROOT_LOGDIR, "experiment" + dt.datetime.now().strftime("%Y%m%d-%H%M%S"))
history = model.fit(
    x_train, y_train,
    epochs=100,                   # n칰mero de 칠pocas
    batch_size=64,               # tama침o de batch
    validation_data=(x_test, y_test),
    verbose=1,
    callbacks=[keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=1),
               early_stopping]
)

# Evaluar
train_loss, train_acc = model.evaluate(x_train, y_train, verbose=0)
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)

print(f"\n游꿢 Resultados TensorFlow:")
print(f"  Training Accuracy: {train_acc:.1%}")
print(f"  Test Accuracy: {test_acc:.1%}")
print(f"  Par치metros totales: {model.count_params():,}")
```

![](../assets/07/02.png)

El modelo logr칩 obtener una precisi칩n del 64.6% para el set de entrenamiento y una precisi칩n
del 54.4% para el set de testeo, con un total de 6.305.802 par치metros totales.

Si bien estos resultados no son malos, se puede ver un claro overfitting del modelo,
teniendo m치s de 10% mejor precisi칩n en el set de entrenamiento, adem치s que la precisi칩n con 
el set de testeo es tan solo ligermante mayor al 50% (el cu치l era el valor inicial).

## Reflexi칩n
En esta actividad aprend칤 ligeramente las diferentes formas en las que se puede "jugar" con un
modelo de keras (y en general) para mejorar los resultados que este devuelva.

Tambi칠n me permiti칩 ver la dificultad de resolver problemas de clasificaci칩n de im치genes con
una aplicaci칩n pr치ctica.

Si bien el modelo logr칩 un resultado mejor al que hab칤a incialmente, para nada se podr칤a decir que
est치 especializado en clasificaci칩n de im치genes; es por esto que un pr칩ximo paso evidente es
el experimentar con distintos tipos de redes neuronales o modelos preentrenados para la clasificaci칩n
de im치genes, y comparar los resultados que estos pueden llegar a obtener frente a los obtenidos
en esta actividad.

## Referencias
[Link al Colab](https://colab.research.google.com/drive/123b4GSv0BRKtVrmxV7SHnayH4p2KKU7k?usp=sharing)