---
title: "Visualizando lo escondido: Entendiendo lo que se encuentra detrás del código"
date: 2025-12-02
---

# Visualizando lo escondido: Entendiendo lo que se encuentra detrás del código

## Contexto
A lo largo del curso hemos tratado con diferentes modelos (redes neuronales multicapa, transformers, CNNs)
y diferentes técnicas que estas aplican (Backpropagation, descenso de gradiente); sin embargo, hemos visto
sólo las aplicaciones de estos elementos sin poner mucho enfasis en lo que hay detrás, su origen matemático.

En este artículo busco vincular algunos de los temas dados en clase con sus explicaciones y orígenes
matemáticos con base en la serie de videos de Neural Networks de 3Blue1Brown.

## Objetivos
- Entender el vínculo entre neuronas y pesos con matrices y tensores
- Relacionar el descenso de gradiente con la configuración de optimizadores
y learning rates
- Comprender matematicamente el algoritmo de Backpropagation

## Desarrollo
En el primero de los videos, se presentan las neuronas de las capas de las redes neuronales como
luces que pueden tener más o menos iluminación, variando entre 0 y 1. Cada una de estas neuronas
conectada con todas las de la capa anterior a través de líneas.

En el curso, primero lo vimos en la introducción a MLP, donde los valores de entrada se veían
mutliplicados por pesos y afectados por los bias, y que eso resultaba, junto con la función de activación,
la salida ("iluminación") que mostraban.

En el segundo de los video nos introducimos en el descendo de gradiente y la función de costo, siendo
la última la manera de determinar si la salida de una red neuronal está en línea con lo que buscamos
o si está dando valores completamente erróneos, y la primera siendo el método que nos permite decrecer
este costo de la manera más rápida. Esto vincula directamente con el uso del learning rate en las
actividades, donde si interpretamos la función de costo como una serie de montañas y valles, aumentar
o decrecer el learning rate indice cúanto podemos movernos en ese terreno; valores muy bajos llegarán
lentamente a un valle local, mientras que valores altos pueden pasarse completamente y no llegar a un
punto concreto.

Otro punto importante de la serie de videos es la explicación y visualización del Backpropagation,
donde en el ejemplo utilizando el dataset de MNIST, frente a un input de 2, se puede calcular la
diferencia entre las probabilidades dados por la red de los otros números (que deberían ser 0) y 
la probabilidad de 2 (que debería ser 1), y con este cálculo influenciar a las neuronas anteriores
a favorecer a 2. Sin embargo, si esto se hiciera con este ejemplo único arbitrario, la red sólo
funcionaría para clasificar el número 2; es por esto que, para ajustar los pesos de las neuronas
anteriores, se toma el peso de todas las neuronas en esa capa y se promedia sobre todo el set
de entrenamiento. Es decir, para todos los números 1, 2, 3, etc..., que se encuentren en el dataset,
todos ellos influenciaran en los pesos de la última capa, los cuales influenciarán en los pesos de las
anteriores y así sucesivamente.

## Reflexión
Si bien el hecho de haber visto ambas partes de la misma realidad me deja con más preguntas que
respuestas en ambos sentidos, siento que elevó de gran manera el aprendizaje al poder conectar
lo que en un inicio parecía magia con algo más llevado a tierra que es la matemática.
- Qué aprendiste, qué mejorarías, próximos pasos

## Referencias
- [Neural networks - 3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)