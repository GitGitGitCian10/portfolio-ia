---
title: "Entrada 04 ‚Äî Validaci√≥n y Selecci√≥n de Modelos"
date: 2025-10-12
---

# Entrada 04 ‚Äî Validaci√≥n y Selecci√≥n de Modelos

## Contexto

En este art√≠culo trabajaremos con un dataset de abandono estudiantil y √©xito
acad√©mico en educaci√≥n superior (Predict Students' Dropout and Academic Success),
buscando predecir el √©xito o abandono de los estudiantes en base caracter√≠sticas
ac√°demicas, familiares, √©tnicas y financieras.

Para esta pr√°ctica utilizaremos modelos de regresi√≥n log√≠stica, regresi√≥n Ridge y
bosque aleatorio para la predicci√≥n y posteriormente realizaremos validaci√≥n
cruzada con t√©cnicas de KFold y StratifiedKFold para evaluar el funcionamiento de
los modelos.

## Objetivos
- Aprender a prevenir data leakage usando pipelines
- Implementar validaci√≥n cruzada (cross-validation) robusta
- Comparar m√∫ltiples modelos de forma sistem√°tica
- Interpretar m√©tricas de estabilidad y selecci√≥n de modelos

## Actividades (con tiempos estimados)
- Setup de herramientas, carga y exploraci√≥n del dataset ‚Äî 10 min
- Preparaci√≥n de datos ‚Äî 3 min
- Implementaci√≥n de validaci√≥n cruzada ‚Äî 10 min
- Comparaci√≥n con diferentes modelos - 15 min
- Optimizaci√≥n de hiperpar√°metros ‚Äî 15 min
- Explicabilidad del modelo ‚Äî 20 min


## Desarrollo

### 1. Setup de herramientas, carga y exploraci√≥n del dataset 

Primeramente instalaremos ucimlrepo, que es donde encontraremos el dataset, e importaremos las
herramientas a utilizar en esta pr√°ctica.

```python linenums="1"
!pip install ucimlrepo

# Importar librer√≠as que vamos a usar
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Para validaci√≥n y selecci√≥n de modelos
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
# Para cargar datos desde UCI ML Repository
from ucimlrepo import fetch_ucirepo
from sklearn.metrics import accuracy_score, classification_report

print("Setup completo!")
```

A continuaci√≥n, cargaremos el dataset, guardaremos las features y las variables objetivo,
y revisaremos ligeramente el dataset y sus caracter√≠sticas.

```python linenums="1"
# Cargar dataset de estudiantes desde UCI
student_data = fetch_ucirepo(id=697)

# Preparar datos
X = student_data.data.features
y = student_data.data.targets

print("Dataset: Student Dropout and Academic Success")
print(f"Estudiantes: {X.shape[0]}, Caracter√≠sticas: {X.shape[1]}")
print(f"Objetivo: Predecir {len(y.columns)} variable(s)")

# Explorar variable objetivo
target_col = y.columns[0]  # Primera columna objetivo
y_series = y[target_col]
print(f"\nVariable objetivo: {target_col}")

# Mapear valores para mejor interpretaci√≥n
target_mapping = {'Dropout': 0, 'Enrolled': 1, 'Graduate': 2}
y_mapped = y_series.map(target_mapping)

# Distribuci√≥n de clases
print("\nDistribuci√≥n de resultados acad√©micos:")
value_counts = y_mapped.value_counts()
for outcome, count in value_counts.items():
    percentage = (count / len(y_mapped)) * 100
    print(f"  {outcome}: {count} estudiantes ({percentage:.1f}%)")

# Ver algunas caracter√≠sticas
print(f"\nPrimeras caracter√≠sticas:")
print(X.columns.tolist()[:10], "...")

# Estad√≠sticas b√°sicas
print(f"\nAge at enrollment:")
if 'Age at enrollment' in X.columns:
    age_col = X['Age at enrollment']
    print(f"  Promedio: {age_col.mean():.1f} a√±os")
    print(f"  Rango: {age_col.min():.0f}-{age_col.max():.0f} a√±os")
```

![](../assets/04-01.png)

Como podemos ver, este dataset cuenta con 4424 muestras con 36 caracter√≠sticas cada una,
tambi√©n se observa que las clases est√°n muy desbalanceadas, con una clase conteniendo 1/6 de
los datos, otra conteniendo 2/6 de los datos y la √∫ltima con los 3/6 restantes.

### 2. Preparaci√≥n de datos

En caso de que las clases finales est√©n guardadas como strings, pasan a valores num√©ricos.

```python linenums="1"
# Preparar variable objetivo como serie simple
# Convertir strings a n√∫meros para sklearn
target_mapping = {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}
reverse_mapping = {'Dropout': 0, 'Enrolled': 1, 'Graduate': 2}

# Si y_series contiene strings, convertir a n√∫meros
if y_series.dtype == 'object':
    y_target = y_series.map(reverse_mapping)
else:
    y_target = y_series

X_features = X       # Features del dataset

print("Datos preparados para validaci√≥n:")
print(f"X shape: {X_features.shape}")
print(f"y shape: {y_target.shape}")
print(f"Clases √∫nicas: {sorted(y_target.unique())}")
print(f"Mapeo: {target_mapping}")
```

![](../assets/04-02.png)

### 3. Implementaci√≥n de validaci√≥n cruzada

Creamos un pipeline que contenga tanto la t√©cnica de estandarizaci√≥n como el modelo seleccionado,
de este modo, se aplicar√° la estandarizaci√≥n unicamente al conjunto de datos de cada pliegue 
(ya sea los de KFold o StratifiedKFold) sin tener en cuenta el conjunto de datos entero previniendo
la fuga de datos.

```python linenums="1"
# === VALIDACI√ìN CRUZADA PARA ESTABILIDAD ===

print("üî¨ VALIDACI√ìN CRUZADA: ¬øQu√© tan estable es nuestro modelo?")

# 1. Crear pipeline robusto para usar en CV
pipeline_robust = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression(max_iter=1000, random_state=42))
])

print("Pipeline creado para validaci√≥n cruzada")
```

Luego realizamos la validaci√≥n cruzada con ambos KFold y visualizamos los resultados.

```python linenums="1"
# 2. Crear KFold b√°sico
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# 3. Evaluar con KFold usando cross_val_score
scores_kfold = cross_val_score(
    pipeline_robust, X_features, y_target, cv=kfold, scoring='accuracy'
)

print(f"\nKFOLD RESULTS:")
print(f"   Scores individuales: {scores_kfold}")
print(f"   Media: {scores_kfold.mean():.4f}")
print(f"   Desviaci√≥n est√°ndar: {scores_kfold.std():.4f}")
print(f"   Resultado: {scores_kfold.mean():.4f} ¬± {scores_kfold.std():.4f}")

# 4. Crear StratifiedKFold (mantiene proporci√≥n de clases)
stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 5. Evaluar con StratifiedKFold
scores_stratified = cross_val_score(
    pipeline_robust, X_features, y_target, cv=stratified_kfold, scoring='accuracy'
)

print(f"\nSTRATIFIED KFOLD RESULTS:")
print(f"   Scores individuales: {scores_stratified}")
print(f"   Media: {scores_stratified.mean():.4f}")
print(f"   Desviaci√≥n est√°ndar: {scores_stratified.std():.4f}")
print(f"   Resultado: {scores_stratified.mean():.4f} ¬± {scores_stratified.std():.4f}")

# 6. Comparar estabilidad (menor desviaci√≥n = m√°s estable)
print(f"\nCOMPARACI√ìN DE ESTABILIDAD:")
if scores_stratified.std() < scores_kfold.std():
    print("   StratifiedKFold es M√ÅS ESTABLE (menor variabilidad)")
    mejor_cv = "StratifiedKFold"
else:
    print("   KFold es M√ÅS ESTABLE (menor variabilidad)")
    mejor_cv = "KFold"

print(f"   Recomendaci√≥n: Usar {mejor_cv} para este dataset")

# 7. Visualizar la distribuci√≥n de scores
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.boxplot([scores_kfold, scores_stratified], labels=['KFold', 'StratifiedKFold'])
plt.title('Distribuci√≥n de Scores - Validaci√≥n Cruzada')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3)
plt.show()
```

![](../assets/04-03.png)

Si bien ambos KFold dieron buenos resultados, nos quedaremos con el StratifiedKFold
dado a su menor desviaci√≥n est√°ndar, que se traduce en mayor estabilidad, y por el
hecho de que las clases se encuentran bastante desbalanceadas y el Stratified mantiene
la proporci√≥n de las clases en cada pliegue.

### 4. Comparaci√≥n con diferentes modelos

En este paso creamos diferentes pipelines para los diferentes modelos de manera similar
al utilizado anteriormente.

```python linenums="1"
# === COMPETENCIA DE MODELOS ===

print("üèÜ TORNEO: ¬øCu√°l modelo funciona mejor para diagn√≥stico m√©dico?")

# 1. Definir candidatos (diferentes algoritmos)
models = {
    'Logistic Regression': Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(max_iter=1000, random_state=42))
    ]),

    # 2. Ridge Classifier (regresi√≥n log√≠stica con regularizaci√≥n L2)
    'Ridge Classifier': Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', RidgeClassifier(alpha=1.0, random_state=42))
    ]),

    # 3. Random Forest (ensemble, no necesita escalado)
    'Random Forest': Pipeline([
        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
    ])
}

print(f"Modelos en competencia: {list(models.keys())}")
```

!!! note "Nota"
    N√≥tese que, para el modelo del Random Forest, no es necesario aplicar estandarizaci√≥n
    debido a la manera en que este funciona; ya que, por ejemplo, ser√≠a lo mismo la decisi√≥n de 
    'calificaci√≥n > 150' que 'calificaci√≥n > 0.75' una vez estandarizado, por esto se omite.

Evaluamos cada modelo con la validaci√≥n cruzada, conseguimos el mejor y visualizamos los
resultados.


```python linenums="1"
# 4. Evaluar cada modelo con validaci√≥n cruzada
print(f"\nEVALUANDO MODELOS CON 5-FOLD CV...")

results = {}
for name, model in models.items():
    print(f"   Evaluando {name}...")

    # Usar StratifiedKFold para mantener balance de clases
    scores = cross_val_score(
        model, X_features, y_target, 
        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
        scoring='accuracy'
    )

    results[name] = scores

    print(f"   {name}: {scores.mean():.4f} ¬± {scores.std():.4f}")
    print(f"      Scores: {[f'{s:.3f}' for s in scores]}")

# 5. Encontrar el mejor modelo
print(f"\nRESULTADOS FINALES:")

# Encontrar modelo con mayor accuracy promedio
best_mean_score = 0
best_model_name = ""

for name, scores in results.items():
    if scores.mean() > best_mean_score:
        best_mean_score = scores.mean()
        best_model_name = name

print(f"GANADOR: {best_model_name}")
print(f"Score: {best_mean_score:.4f}")

# 6. An√°lisis detallado de estabilidad
print(f"\nAN√ÅLISIS DE ESTABILIDAD:")
for name, scores in results.items():
    stability = scores.std()

    if stability < 0.02:
        status = "MUY ESTABLE"
    elif stability < 0.05:
        status = "ESTABLE"
    else:
        status = "INESTABLE"

    print(f"   {name}: {status} (std: {stability:.4f})")

# 7. Visualizaci√≥n comparativa
plt.figure(figsize=(12, 6))

# Boxplot de distribuci√≥n de scores
plt.subplot(1, 2, 1)
plt.boxplot([results[name] for name in models.keys()], 
           labels=[name.split()[0] for name in models.keys()])
plt.title('Distribuci√≥n de Accuracy por Modelo')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3)

# Barplot de medias con error bars
plt.subplot(1, 2, 2)
names = list(models.keys())
means = [results[name].mean() for name in names]
stds = [results[name].std() for name in names]

plt.bar(range(len(names)), means, yerr=stds, capsize=5)
plt.xticks(range(len(names)), [name.split()[0] for name in names])
plt.title('Accuracy Promedio ¬± Desviaci√≥n Est√°ndar')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

![](../assets/04-04.png)

### 5. Optimizaci√≥n de hiperpar√°metros

```python linenums="1"
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Seleccionar el mejor modelo de la competencia anterior
best_model_base = models[best_model_name]

print(f"Optimizando hiperpar√°metros para: {best_model_name}")

# Definir espacio de b√∫squeda de hiperpar√°metros
if 'Random Forest' in best_model_name:
    param_grid = {
        'classifier__n_estimators': [50, 100, 200],
        'classifier__max_depth': [None, 10, 20, 30],
        'classifier__min_samples_split': [2, 5, 10]
    }
elif 'Logistic' in best_model_name:
    param_grid = {
        'classifier__C': [0.1, 1, 10, 100],
        'classifier__max_iter': [1000, 2000]
    }
else:  # Ridge
    param_grid = {
        'classifier__alpha': [0.1, 1, 10, 100]
    }

# M√âTODO 1: GridSearchCV (b√∫squeda exhaustiva)
print("\nM√©todo 1: GridSearchCV (b√∫squeda exhaustiva)")
grid_search = GridSearchCV(
    best_model_base,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_features, y_target)

print(f"Mejores par√°metros (Grid): {grid_search.best_params_}")
print(f"Mejor score (Grid): {grid_search.best_score_:.4f}")

# M√âTODO 2: RandomizedSearchCV (b√∫squeda aleatoria, m√°s eficiente)
print("\nM√©todo 2: RandomizedSearchCV (b√∫squeda aleatoria)")
random_search = RandomizedSearchCV(
    best_model_base,
    param_grid,
    n_iter=20,  # Solo 20 combinaciones aleatorias
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

random_search.fit(X_features, y_target)

print(f"Mejores par√°metros (Random): {random_search.best_params_}")
print(f"Mejor score (Random): {random_search.best_score_:.4f}")

# Comparar eficiencia
print(f"\nComparaci√≥n de eficiencia:")
print(f"GridSearch prob√≥: {len(grid_search.cv_results_['params'])} combinaciones")
print(f"RandomSearch prob√≥: {len(random_search.cv_results_['params'])} combinaciones")

# Evaluar modelo final optimizado
final_model = grid_search.best_estimator_
final_scores = cross_val_score(final_model, X_features, y_target, cv=5)
print(f"\nModelo final optimizado: {final_scores.mean():.4f} ¬± {final_scores.std():.4f}")
```

![](../assets/04-05.png)

### 6. Explicabilidad del modelo

```python linenums="1"
# Usar Random Forest para explicabilidad (si no gan√≥, crearlo)
if 'Random Forest' not in best_model_name:
    # Crear Random Forest espec√≠fico para explicabilidad
    # Random Forest no necesita escalado, as√≠ que lo omitimos para simplicidad
    rf_model = Pipeline([
        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
    ])
    rf_model.fit(X_features, y_target)
    print("Creado Random Forest espec√≠fico para an√°lisis de explicabilidad")
else:
    rf_model = final_model
    print("Usando el modelo ganador para explicabilidad")

# Verificar estructura del pipeline
print(f"Componentes del pipeline: {list(rf_model.named_steps.keys())}")

# 1. FEATURE IMPORTANCE - ¬øQu√© caracter√≠sticas son m√°s importantes?
feature_names = X_features.columns
importances = rf_model.named_steps['classifier'].feature_importances_

# Crear DataFrame para mejor visualizaci√≥n
feature_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
}).sort_values('importance', ascending=False)

print("\nTOP 10 CARACTER√çSTICAS M√ÅS IMPORTANTES:")
for i, row in feature_importance_df.head(10).iterrows():
    print(f"{row['feature']}: {row['importance']:.4f}")

# Visualizar importancia de caracter√≠sticas
plt.figure(figsize=(10, 8))
top_features = feature_importance_df.head(15)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('Importancia')
plt.title('Top 15 Caracter√≠sticas M√°s Importantes para Predecir √âxito Estudiantil')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# 2. AN√ÅLISIS POR CATEGOR√çAS - Agrupar caracter√≠sticas relacionadas
academic_features = [col for col in feature_names if any(word in col.lower() 
                    for word in ['grade', 'units', 'curricular', 'semester'])]
demographic_features = [col for col in feature_names if any(word in col.lower() 
                       for word in ['age', 'gender', 'nationality', 'marital'])]
economic_features = [col for col in feature_names if any(word in col.lower() 
                    for word in ['scholarship', 'debt', 'fee', 'tuition'])]

def calculate_category_importance(features, importance_df):
    if not features:
        return 0
    category_importance = importance_df[importance_df['feature'].isin(features)]['importance'].sum()
    return category_importance

academic_importance = calculate_category_importance(academic_features, feature_importance_df)
demographic_importance = calculate_category_importance(demographic_features, feature_importance_df)
economic_importance = calculate_category_importance(economic_features, feature_importance_df)

print(f"\nIMPORTANCIA POR CATEGOR√çAS:")
print(f"Factores acad√©micos: {academic_importance:.4f}")
print(f"Factores demogr√°ficos: {demographic_importance:.4f}")
print(f"Factores econ√≥micos: {economic_importance:.4f}")

# 3. INTERPRETACI√ìN PR√ÅCTICA - ¬øQu√© significa esto?
print(f"\nINTERPRETACI√ìN PARA INTERVENCIONES:")
print(f"La caracter√≠stica m√°s importante es: {feature_importance_df.iloc[0]['feature']}")
print(f"Esto sugiere que para reducir abandono estudiantil debemos enfocarnos en:")

# Generar recomendaciones basadas en las top features
top_3_features = feature_importance_df.head(3)['feature'].tolist()
for i, feature in enumerate(top_3_features, 1):
    print(f"{i}. Monitorear y mejorar: {feature}")

# 4. PREDICCI√ìN INDIVIDUAL - ¬øPor qu√© un estudiante espec√≠fico est√° en riesgo?
print(f"\nAN√ÅLISIS DE ESTUDIANTE INDIVIDUAL (ejemplo):")
student_idx = 0
student_data = X_features.iloc[student_idx:student_idx+1]
prediction = rf_model.predict(student_data)[0]
prediction_proba = rf_model.predict_proba(student_data)[0]

# Definir mapeo localmente para esta secci√≥n
outcome_mapping = {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}

# Manejar si prediction es string o n√∫mero
if isinstance(prediction, str):
    predicted_outcome = prediction
else:
    predicted_outcome = outcome_mapping[prediction]

print(f"Estudiante #{student_idx}:")
print(f"Predicci√≥n: {predicted_outcome}")
print(f"Probabilidades:")
for i, prob in enumerate(prediction_proba):
    outcome_name = outcome_mapping[i]
    print(f"  {outcome_name}: {prob:.3f}")

# Mostrar las caracter√≠sticas m√°s importantes de este estudiante
student_features = pd.DataFrame({
    'feature': feature_names,
    'value': student_data.iloc[0].values,
    'importance': importances
}).sort_values('importance', ascending=False)

print(f"\nTop 5 caracter√≠sticas que influyen en esta predicci√≥n:")
for i, row in student_features.head(5).iterrows():
    print(f"{row['feature']}: {row['value']:.2f} (importancia: {row['importance']:.4f})")

# 5. VISUALIZACI√ìN DE √ÅRBOLES INDIVIDUALES
print(f"\nVISUALIZACI√ìN DE √ÅRBOLES DEL RANDOM FOREST:")

# Instalar graphviz si no est√° disponible
try:
    from sklearn.tree import export_graphviz, plot_tree, export_text
    import matplotlib.pyplot as plt

    # Obtener algunos √°rboles del bosque
    forest = rf_model.named_steps['classifier']
    n_trees_to_show = min(3, len(forest.estimators_))

    print(f"Mostrando {n_trees_to_show} √°rboles de {len(forest.estimators_)} totales")

    # Visualizar √°rboles con plot_tree (m√°s simple)
    fig, axes = plt.subplots(1, n_trees_to_show, figsize=(25, 12))
    if n_trees_to_show == 1:
        axes = [axes]

    for i in range(n_trees_to_show):
        tree = forest.estimators_[i]

        # Limitar profundidad para que sea legible
        plot_tree(tree, 
                 ax=axes[i],
                 feature_names=list(feature_names),  # Usar todos los nombres de caracter√≠sticas
                 class_names=list(outcome_mapping.values()),
                 filled=True,
                 max_depth=3,  # Limitar profundidad
                 fontsize=6)  # Fuente m√°s peque√±a para que quepa

        axes[i].set_title(f'√Årbol {i+1} (profundidad m√°x: 3)', fontsize=12)

    plt.tight_layout()
    plt.show()

    # Informaci√≥n sobre la estructura de los √°rboles
    print(f"\nESTAD√çSTICAS DE LOS √ÅRBOLES:")
    depths = [tree.get_depth() for tree in forest.estimators_[:5]]
    n_nodes = [tree.tree_.node_count for tree in forest.estimators_[:5]]

    print(f"Profundidad promedio (primeros 5 √°rboles): {sum(depths)/len(depths):.1f}")
    print(f"N√∫mero promedio de nodos (primeros 5): {sum(n_nodes)/len(n_nodes):.1f}")

    # Mostrar un √°rbol muy simple por texto
    print(f"\nEJEMPLO DE REGLAS DE DECISI√ìN (√Årbol 1, simplificado):")
    tree_rules = export_text(forest.estimators_[0], 
                           feature_names=list(feature_names),
                           max_depth=2)
    print(tree_rules[:500] + "..." if len(tree_rules) > 500 else tree_rules)

except ImportError:
    print("Para visualizar √°rboles, instala: pip install graphviz")
    print("Alternativamente, mostramos la estructura del bosque:")

    forest = rf_model.named_steps['classifier']
    print(f"Random Forest contiene {len(forest.estimators_)} √°rboles")
    print(f"Cada √°rbol fue entrenado con {forest.max_features_} caracter√≠sticas aleatorias")

    # Estad√≠sticas b√°sicas sin visualizaci√≥n
    if len(forest.estimators_) > 0:
        depths = [tree.get_depth() for tree in forest.estimators_[:5]]
        print(f"Profundidad promedio: {sum(depths)/len(depths):.1f}")

# 6. DIVERSIDAD DEL BOSQUE
print(f"\nDIVERSIDAD EN EL RANDOM FOREST:")
print("El poder del Random Forest viene de la diversidad de sus √°rboles:")
print("- Cada √°rbol ve una muestra diferente de datos (bootstrap)")
print("- Cada split considera solo un subconjunto aleatorio de caracter√≠sticas")
print("- La predicci√≥n final es el voto mayoritario de todos los √°rboles")

# Mostrar diferencias en predicciones individuales
student_sample = X_features.iloc[0:1]
individual_predictions = []

# Preparar datos dependiendo de si el modelo tiene scaler o no
if 'scaler' in rf_model.named_steps:
    # Modelo con scaler
    scaled_sample = rf_model.named_steps['scaler'].transform(student_sample)
    print("Usando datos escalados para √°rboles individuales")
else:
    # Modelo sin scaler (ej: Random Forest sin preprocesamiento)
    scaled_sample = student_sample.values
    print("Usando datos sin escalar para √°rboles individuales")

for i, tree in enumerate(forest.estimators_[:5]):
    tree_pred = tree.predict(scaled_sample)[0]
    individual_predictions.append(tree_pred)

print(f"\nPredicciones de √°rboles individuales para el Estudiante #0:")
for i, pred in enumerate(individual_predictions):
    pred_name = outcome_mapping[pred] if isinstance(pred, int) else pred
    print(f"  √Årbol {i+1}: {pred_name}")

final_pred = max(set(individual_predictions), key=individual_predictions.count)
final_pred_name = outcome_mapping[final_pred] if isinstance(final_pred, int) else final_pred
print(f"Predicci√≥n final (voto mayoritario): {final_pred_name}")
```

![](../assets/04-06.png)

![](../assets/04-07.png)

![](../assets/04-08.png)

![](../assets/04-09.png)


### Preguntas posteriores

- ¬øQu√© es data leakage y por qu√© es peligroso?

Data leakage se produce cuando el modelo accede a informaci√≥n de la variable objetivo o a informaci√≥n del
set de validaci√≥n en el entrenamiento, esto resulta en el modelo mostrando un rendimiento muy alto
en el entrenamiento y validaci√≥n, pero uno muy pobre cuando se lleva a un ambiente real.

- ¬øCu√°ndo usar KFold vs StratifiedKFold?

Se utiliza KFold cuando las clases en la variable objetivo est√°n balanceadas, mientras que es preferible usar StratifiedKFold cuando las clases no est√°n balanceadas, ya que intentar√°
mantener la proporci√≥n del dataset original.

- ¬øC√≥mo interpretar "95.2% ¬± 2.1%" en cross-validation?

95.2% en la validaci√≥n cruzada significa que, el modelo a trav√©s de todos los pliegues, logr√≥ un rendimiento
promedio de 95.2% en la m√©trica utilizada; mientras que el 2.1% indica la desviaci√≥n est√°ndar, que determina
la estabilidad e indica qu√© tanto variaron los resultados del promedio a trav√©s de todos los pliegues.

- ¬øPor qu√© Random Forest no necesita StandardScaler?

Random Forest no necesita StandardScaler ya que este se basa en comparaciones y no toma en cuenta la magnitud
de los valores, para el Random Forest es igual la comparaci√≥n 'edad > 18' que 'edad > 0.22' (suponiendo que 18 se
vuelve 0.18 luego de la estandarizaci√≥n), y es por eso que no requiere StandardScaler.

- En diagn√≥stico m√©dico, ¬øprefieres un modelo con 98% accuracy pero inestable, o 95% accuracy pero muy estable?

Teniendo en cuenta un contexto m√©dico, donde errores grandes pueden costar la vida de un paciente, es prefereible
un modelo con ligeramente menor accuracy pero, a su vez, mucha menor varianza. En este sentido, es mejor unos
cuantos diagnosticos ligeramente inexactos que unos pares de diagnosticos completamente err√≥neos.

#### ¬øQu√© significan las m√©tricas de validaci√≥n?
- Cross-Validation: T√©cnica que divide los datos en K partes para entrenar y evaluar m√∫ltiples veces.
- Accuracy promedio: La estimaci√≥n de rendimiento esperado en datos nuevos.
- Desviaci√≥n est√°ndar: Indica qu√© tan estable es el modelo entre diferentes divisiones de datos.
- StratifiedKFold: Mantiene la proporci√≥n de clases en cada fold, especialmente importante en datasets desbalanceados.

#### ¬øCu√°ndo usar cada m√©todo?
- GridSearchCV cuando tienes pocos hiperpar√°metros y suficiente tiempo de c√≥mputo.
- RandomizedSearchCV cuando tienes muchos hiperpar√°metros o un tiempo limitado.
- Pipeline + SearchCV siempre previene data leakage autom√°ticamente.
- cross_val_score en el resultado final valida que la optimizaci√≥n no caus√≥ overfitting.

#### ¬øPor qu√© es importante la explicabilidad?
- Confianza: Los educadores necesitan entender por qu√© el modelo predice abandono.
- Intervenciones: Conociendo las caracter√≠sticas importantes permite crear estrategias espec√≠ficas.
- Bias detection: La explicabilidad ayuda a detectar bias en el modelo.
- Regulaciones: Muchos contextos requieren modelos interpretables por ley.
- Mejora continua: Entender el modelo ayuda a iterar futuras versiones.

## Evidencias
- [Link al Colab](https://colab.research.google.com/drive/1CUSprz-0sMtYtH-Rjuq2ve7nYMmepkRc?usp=sharing)

## Reflexi√≥n
Conidero que la validaci√≥n cruzada pasar√° a ser bastante importante cuando no solo precisemos
modelos con buen rendimiento sino que tambi√©n sean estables y consistentes en sus resultados.
Un pr√≥ximo paso previsible ser√≠a utilizar validaci√≥n cruzada junto con train_test_split,
utilizando los datos de entrenamiento para la validaci√≥n cruzada y teniendo los √∫ltimos
datos de test para una prueba final.

Algo importante a tener en cuenta es la existencia del data leakage y tenerlo en cuenta a la
hora de entrenar modelos para que estos puedan funcionar correctamente. Otro punto de inter√©s
ser√≠a maneras de eivtarlo adem√°s del uso de pipelines.