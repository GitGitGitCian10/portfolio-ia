---
title: "Entrada 05 â€” Clustering y PCA"
date: 2025-10-13
---

# Entrada 05 â€” Clustering y PCA

## Contexto
Breve descripciÃ³n del tema/actividad.

## Objetivos
- Identificar de 3 a 5 segmentos de clientes distintos usando K-Means
- Aplicar diferentes tÃ©cnicas de normalizaciÃ³n
- Usar PCA para reducciÃ³n de dimensionalidad y visualizaciÃ³n 
- Comparar PCA con mÃ©todos de selecciÃ³n de features 
- Interpretar resultados desde perspectiva de negocio

## Actividades (con tiempos estimados)
- Entendimiento del negocio â€” 5 min
- Entendimiento de los datos â€” 25 min
- PreparaciÃ³n de los datos â€” 45 min
- Modelado y clustering â€” 30 min
- EvaluaciÃ³n y resumen â€” 25 min

## Desarrollo

### 1. Entendimiento del negocio

Los centros comerciales buscan mejores maneras de llegar a sus clientes, ya sea a travÃ©s de
marketing personalizado, ofertas especÃ­icas o mayor accesibilidad a productos relevantes
para los clientes.

Para lograr esto, el negocio debe conocer bien a sus cliente y, sobre todo, ser capaz de
agruparlos de manera que maximize la cantidad de clientes afectados cuando intervenga con
un segmento de los clientes con los objetivos dichos en el pÃ¡rrafo anterior.

### 2. Entendimiento de los datos

El dataset de Mall Customer Segmentation de Kaggle incluye informaciÃ³n de 200 clientes
en centro comerciales con pocos pero relevantes datos para le negocio.

#### Columnas:

- CustomerID: nÃºmero identificador del cliente
- Gender: gÃ©nero del cliente, 'male' o 'female'
- Age: edad del cliente, nÃºmero entero mayor a 18
- Annual Income (k$): ingreso anual representado en 1000 de dolares
- Spending Score: puntuaciÃ³n asignada por el centro comercial basada
en comportamiento y naturaleza de compra, nÃºmero del 1 al 100

```python linenums="1"
print("INFORMACIÃ“N DEL DATASET:")
print(f"Shape: {df_customers.shape[0]} filas, {df_customers.shape[1]} columnas")
print(f"Columnas: {list(df_customers.columns)}")
print(f"Memoria: {df_customers.memory_usage(deep=True).sum() / 1024:.1f} KB")

print(f"\nPRIMERAS 5 FILAS:")
df_customers.head()
```

![](../assets/05-01.png)


```python linenums="1"
# === ANÃLISIS DE GÃ‰NERO ===
print("DISTRIBUCIÃ“N POR GÃ‰NERO:")
gender_counts = df_customers['Genre'].value_counts()
print(gender_counts)
print(f"\nPorcentajes:")
for gender, count in gender_counts.items():
    pct = (count / len(df_customers) * 100)
    print(f"   {gender}: {pct:.1f}%")![alt text](image.png)
```

![](../assets/05-02.png)

```python linenums="1"
# === ESTADÃSTICAS DE VARIABLES DE SEGMENTACIÃ“N ===
numeric_vars = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']

print("ESTADÃSTICAS CLAVE:")
print(df_customers[numeric_vars].describe().round(2))

print(f"\nRANGOS OBSERVADOS:")
for var in numeric_vars:
    min_val, max_val = df_customers[var].min(), df_customers[var].max()
    mean_val = df_customers[var].mean()
    print(f"   {var}: {min_val:.0f} - {max_val:.0f} (promedio: {mean_val:.1f})")
```

![](../assets/05-03.png)

```python linenums="1"
# === DETECCIÃ“N DE OUTLIERS USANDO IQR ===
print("DETECCIÃ“N DE OUTLIERS:")

outlier_cols = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']

for col in outlier_cols:
    Q1 = df_customers[col].quantile(0.25)
    Q3 = df_customers[col].quantile(0.75)
    IQR = Q3 - Q1

    # Calcular lÃ­mites
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Encontrar outliers
    outliers = df_customers[(df_customers[col] < lower_bound) |
                           (df_customers[col] > upper_bound)]

    print(f"   {col}: {len(outliers)} outliers ({len(outliers)/len(df_customers)*100:.1f}%)")
    print(f"      LÃ­mites normales: {lower_bound:.1f} - {upper_bound:.1f}")
```

![](../assets/05-04.png)

```python linenums="1"
# === IMPORTS PARA VISUALIZACIÃ“N ===
import matplotlib.pyplot as plt
import seaborn as sns

# Configurar estilo
plt.style.use('default')
sns.set_palette("husl")

# === HISTOGRAMAS DE VARIABLES PRINCIPALES ===
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
fig.suptitle('Distribuciones de Variables Clave', fontsize=14, fontweight='bold')

vars_to_plot = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']

for i, (var, color) in enumerate(zip(vars_to_plot, colors)):
    axes[i].hist(df_customers[var], bins=20, alpha=0.7, color=color, edgecolor='black')
    axes[i].set_title(f'{var}')
    axes[i].set_xlabel(var)
    axes[i].set_ylabel('Frecuencia')
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

![](../assets/05-05.png)

```python linenums="1"
# === SCATTER PLOTS PARA RELACIONES CLAVE ===
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
fig.suptitle('Relaciones Entre Variables', fontsize=14, fontweight='bold')

# Age vs Income
axes[0].scatter(df_customers['Age'], df_customers['Annual Income (k$)'],
                alpha=0.6, color='#96CEB4', s=50)
axes[0].set_xlabel('Age (aÃ±os)')
axes[0].set_ylabel('Annual Income (k$)')
axes[0].set_title('Age vs Income')
axes[0].grid(True, alpha=0.3)

# Income vs Spending Score â­ CLAVE PARA SEGMENTACIÃ“N
axes[1].scatter(df_customers['Annual Income (k$)'], df_customers['Spending Score (1-100)'],
                alpha=0.6, color='#FFEAA7', s=50)
axes[1].set_xlabel('Annual Income (k$)')
axes[1].set_ylabel('Spending Score (1-100)')
axes[1].set_title('Income vs Spending Score (CLAVE)')
axes[1].grid(True, alpha=0.3)

# Age vs Spending Score
axes[2].scatter(df_customers['Age'], df_customers['Spending Score (1-100)'],
                alpha=0.6, color='#DDA0DD', s=50)
axes[2].set_xlabel('Age (aÃ±os)')
axes[2].set_ylabel('Spending Score (1-100)')
axes[2].set_title('Age vs Spending Score')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

![](../assets/05-06.png)

```python linenums="1"
# === MATRIZ DE CORRELACIÃ“N ===
correlation_vars = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']
corr_matrix = df_customers[correlation_vars].corr()

print("MATRIZ DE CORRELACIÃ“N:")
print(corr_matrix.round(3))

# Visualizar matriz de correlaciÃ³n
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='RdYlBu_r', center=0,
            fmt='.3f', linewidths=0.5, square=True)
plt.title('Matriz de CorrelaciÃ³n - Mall Customers')
plt.tight_layout()
plt.show()

print(f"\nCORRELACIÃ“N MÃS FUERTE:")
# Encontrar la correlaciÃ³n mÃ¡s alta (excluyendo diagonal)
corr_flat = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
max_corr = corr_flat.stack().idxmax()
max_val = corr_flat.stack().max()
print(f"   {max_corr[0]} â†” {max_corr[1]}: {max_val:.3f}")
```

![](../assets/05-07.png)

```python linenums="1"
# === COMPARACIÃ“N ESTADÃSTICAS POR GÃ‰NERO ===
print("ANÃLISIS COMPARATIVO POR GÃ‰NERO:")

gender_stats = df_customers.groupby('Genre')[numeric_vars].agg(['mean', 'std']).round(2)
print(gender_stats)

print(f"\nINSIGHTS POR GÃ‰NERO:")
for var in numeric_vars:
    male_avg = df_customers[df_customers['Genre'] == 'Male'][var].mean()
    female_avg = df_customers[df_customers['Genre'] == 'Female'][var].mean()
    if male_avg > female_avg:
        higher = "Hombres"
        diff = male_avg - female_avg
    else:
        higher = "Mujeres"
        diff = female_avg - male_avg
    print(f"   {var}: {higher} tienen promedio mÃ¡s alto (diferencia: {diff:.1f})")
```

![](../assets/05-08.png)

En base a lo observado:
- Variable con mayor variabilidad: Annual Income
- Â¿Existe correlaciÃ³n fuerte entre alguna variable? Entre Annual Income y Spending Score
- Â¿QuÃ© variable tiene mÃ¡s outliers? Annual Income
- Â¿Los hombres y mujeres tienen patrones diferentes? Las mujeres suelen tener mayor Spending Score y menor Annual Income que los hombres
- Â¿QuÃ© insight es mÃ¡s relevante para el anÃ¡lisis? Matriz de correlaciÃ³n
- Â¿QuÃ© 2 variables serÃ¡n mÃ¡s importantes para clustering? Annual Income y Spending Score

PreparaciÃ³n para clustering:
- Â¿QuÃ© relaciÃ³n entre Income y Spending Score observas? Practicamente exclusivamente los individuos cercanos al promedio de Annual Income (entre 40 y 70) tiene un Spending Score cercano al promedio (entre 40 y 60). TambiÃ©n existen 'grupos' de individuos en las 4 combinaciones de Annual Income (Bajo/Alto) y Spending Score (Bajo/Alto).
- Â¿Puedes imaginar grupos naturales de clientes? Cliente que ganan poco y compran pocos; ganan poco y compran mucho; ganan mucho y compran poco; ganan mucho y compran mucho; y ganan el promedio y compran lo promedio.

```python linenums="1"
# === ANÃLISIS DE COLUMNAS DISPONIBLES ===
print("ANÃLISIS DE COLUMNAS PARA CLUSTERING:")
print(f"   Todas las columnas: {list(df_customers.columns)}")
print(f"   NumÃ©ricas: {df_customers.select_dtypes(include=[np.number]).columns.tolist()}")
print(f"   CategÃ³ricas: {df_customers.select_dtypes(include=[object]).columns.tolist()}")

# Identificar quÃ© excluir y quÃ© incluir
exclude_columns = ['CustomerID']  # ID no aporta informaciÃ³n
numeric_columns = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']
categorical_columns = ['Genre']

print(f"\nSELECCIÃ“N DE FEATURES:")
print(f"   Excluidas: {exclude_columns} (no informativas)")
print(f"   NumÃ©ricas: {numeric_columns}")
print(f"   CategÃ³ricas: {categorical_columns} (codificaremos)")
```

![](../assets/05-09.png)

```python linenums="1"
# === IMPORT ONEHOTENCODER ===
from sklearn.preprocessing import OneHotEncoder

print("CODIFICACIÃ“N DE VARIABLES CATEGÃ“RICAS CON SKLEARN:")
print("Usaremos OneHotEncoder en lugar de pd.get_dummies() por varias razones:")
print("   IntegraciÃ³n perfecta con pipelines de sklearn")
print("   Manejo automÃ¡tico de categorÃ­as no vistas en nuevos datos")
print("   Control sobre nombres de columnas y comportamiento")
print("   Consistencia con el ecosistema de machine learning")

# Crear y configurar OneHotEncoder
encoder = OneHotEncoder(sparse_output=False)

# Ajustar y transformar Genre
genre_data = df_customers[['Genre']]  # Debe ser 2D para sklearn
genre_encoded_array = encoder.fit_transform(genre_data)  # MÃ©todo para ajustar y transformar

# Obtener nombres de las nuevas columnas
feature_names = encoder.get_feature_names_out(['Genre'])  # MÃ©todo para obtener nombres de las features
genre_encoded = pd.DataFrame(genre_encoded_array, columns=feature_names)

print(f"\nRESULTADO DE CODIFICACIÃ“N:")
print(f"   CategorÃ­as originales: {df_customers['Genre'].unique()}")
print(f"   Columnas generadas: {list(genre_encoded.columns)}")
print(f"   Shape: {genre_data.shape} â†’ {genre_encoded.shape}")

# Mostrar ejemplo de codificaciÃ³n
print(f"\nEJEMPLO DE TRANSFORMACIÃ“N:")
comparison = pd.concat([
    df_customers['Genre'].head().reset_index(drop=True),
    genre_encoded.head()
], axis=1)
print(comparison)
```

![](../assets/05-10.png)

```python linenums="1"
# === CREACIÃ“N DEL DATASET FINAL ===
# Combinar variables numÃ©ricas + categÃ³ricas codificadas
X_raw = pd.concat([
    df_customers[numeric_columns],
    genre_encoded
], axis=1)

print("DATASET FINAL PARA CLUSTERING:")
print(f"   Shape: {X_raw.shape}")
print(f"   Columnas: {list(X_raw.columns)}")
print(f"   Variables numÃ©ricas: {numeric_columns}")
print(f"   Variables categÃ³ricas codificadas: {list(genre_encoded.columns)}")
print(f"   Total features: {X_raw.shape[1]} (3 numÃ©ricas + 2 categÃ³ricas binarias)")
print(f"   Memoria: {X_raw.memory_usage(deep=True).sum() / 1024:.1f} KB")
```

![](../assets/05-11.png)

```python linenums="1"
# === VERIFICACIONES ANTES DE CONTINUAR ===
print("VERIFICACIÃ“N DE CALIDAD:")

# 1. Datos faltantes
missing_data = X_raw.isnull().sum()
print(f"\nDATOS FALTANTES:")
if missing_data.sum() == 0:
    print("   PERFECTO! No hay datos faltantes")
else:
    for col, missing in missing_data.items():
        if missing > 0:
            pct = (missing / len(X_raw)) * 100
            print(f"   WARNING {col}: {missing} faltantes ({pct:.1f}%)")

# 2. Vista previa
print(f"\nVISTA PREVIA DEL DATASET:")
print(X_raw.head())

# 3. Tipos de datos
print(f"\nTIPOS DE DATOS:")
print(X_raw.dtypes)
```

![](../assets/05-12.png)

```python linenums="1"
# === ANÃLISIS DE ESCALAS ===
print("ANÃLISIS DE ESCALAS - Â¿Por quÃ© necesitamos normalizaciÃ³n?")

print(f"\nESTADÃSTICAS POR VARIABLE:")
for col in X_raw.columns:
    if X_raw[col].dtype in ['int64', 'float64']:  # Solo numÃ©ricas
        min_val = X_raw[col].min()
        max_val = X_raw[col].max()
        mean_val = X_raw[col].mean()
        std_val = X_raw[col].std()

        print(f"\n   {col}:")
        print(f"      Rango: {min_val:.1f} - {max_val:.1f}")
        print(f"      Media: {mean_val:.1f}")
        print(f"      DesviaciÃ³n: {std_val:.1f}")

# Guardar para prÃ³ximas fases
feature_columns = list(X_raw.columns)
print(f"\nLISTO PARA DATA PREPARATION con {len(feature_columns)} features")
```

![](../assets/05-13.png)

ANÃLISIS DE LAS ESTADÃSTICAS - COMPLETA:
- Â¿QuÃ© variable tiene el rango mÃ¡s amplio? Annual Income
- Â¿CuÃ¡l es la distribuciÃ³n de gÃ©nero en el dataset? Uniforme
- Â¿QuÃ© variable muestra mayor variabilidad (std)? Annual Income
- Â¿Los clientes son jÃ³venes o mayores en promedio? Mayores, alrededor de 40 aÃ±os
- Â¿El income promedio sugiere quÃ© clase social? Clase media o media baja
- Â¿Por quÃ© la normalizaciÃ³n serÃ¡ crÃ­tica aca? SerÃ¡ crÃ­tica ya que hay variables con valores 
de hasta 100 (Spending Score y Annual Income) y otras que su mÃ¡ximo es sÃ³lo 1 (GÃ©nero) pero 
estas Ãºltimas pueden tener un aporte similar o mayor a las primeras en el resultado

### 3. PreparaciÃ³n de los datos

Habiendo preparado la informaciÃ³n para normalizar, utilizaremos 3 diferentes tÃ©cnicas:
MinMaxScaler, StandardScaler y RobustScaler

```python linenums="1"
# === CREAR Y APLICAR LOS 3 SCALERS ===
scalers = {
    'MinMax': MinMaxScaler(),        # Escala a rango [0,1]
    'Standard': StandardScaler(),      # Media=0, std=1
    'Robust': RobustScaler()         # Usa mediana y IQR, robusto a outliers
}

# Aplicar cada scaler
X_scaled = {}
for name, scaler in scalers.items():
    X_scaled[name] = scaler.fit_transform(X_raw)  # MÃ©todo para entrenar y transformar
    print(f"{name}Scaler aplicado: {X_scaled[name].shape}")

print(f"\nTenemos 3 versiones escaladas de los datos para comparar")
```

```python linenums="1"
# === COMPARACIÃ“N VISUAL CON BOXPLOTS ===
fig, axes = plt.subplots(1, 4, figsize=(16, 4))
fig.suptitle('ComparaciÃ³n de Scalers - Boxplots', fontsize=14, fontweight='bold')

# Datos originales
axes[0].boxplot([X_raw[col] for col in X_raw.columns], tick_labels=X_raw.columns)
axes[0].set_title('Original')
axes[0].tick_params(axis='x', rotation=45)

# Datos escalados
for i, (name, X_scaled_data) in enumerate(X_scaled.items(), 1):
    axes[i].boxplot([X_scaled_data[:, j] for j in range(X_scaled_data.shape[1])],
                    tick_labels=X_raw.columns)
    axes[i].set_title(f'{name}')
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

print("Observa cÃ³mo cada scaler ajusta las escalas de forma diferente")
```

![](../assets/05-14.png)

```python linenums="1"
# === COMPARAR DISTRIBUCIONES DE UNA VARIABLE ===
# Vamos a analizar 'Annual Income (k$)' en detalle
income_col_idx = 1  # PosiciÃ³n de Annual Income

fig, axes = plt.subplots(1, 4, figsize=(16, 4))
fig.suptitle('Annual Income: Original vs Scalers', fontsize=14, fontweight='bold')

# Original
axes[0].hist(X_raw.iloc[:, income_col_idx], bins=20, alpha=0.7, color='gray', edgecolor='black')
axes[0].set_title('Original')
axes[0].set_xlabel('Annual Income (k$)')

# Escalados
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
for i, ((name, X_scaled_data), color) in enumerate(zip(X_scaled.items(), colors), 1):
    axes[i].hist(X_scaled_data[:, income_col_idx], bins=20, alpha=0.7, color=color, edgecolor='black')
    axes[i].set_title(f'{name}')
    axes[i].set_xlabel('Annual Income (escalado)')

plt.tight_layout()
plt.show()

print("Â¿Notas cÃ³mo cambia la forma de la distribuciÃ³n?")
```

![](../assets/05-15.png)

```python linenums="1"
# === ESTADÃSTICAS DESPUÃ‰S DEL SCALING ===
print("ESTADÃSTICAS POST-SCALING (Annual Income):")

# Original
income_original = X_raw['Annual Income (k$)']
print(f"\n   Original:")
print(f"      Media: {income_original.mean():.1f}")
print(f"      Std:   {income_original.std():.1f}")
print(f"      Min:   {income_original.min():.1f}")
print(f"      Max:   {income_original.max():.1f}")

# Escalados
for name, X_scaled_data in X_scaled.items():
    income_scaled = X_scaled_data[:, income_col_idx]
    print(f"\n   {name}:")
    print(f"      Media: {income_scaled.mean():.3f}")
    print(f"      Std:   {income_scaled.std():.3f}")
    print(f"      Min:   {income_scaled.min():.3f}")
    print(f"      Max:   {income_scaled.max():.3f}")

print(f"\nOBSERVACIONES:")
print(f"   MinMaxScaler â†’ Rango [0,1]")
print(f"   StandardScaler â†’ Media=0, Std=1")
print(f"   RobustScaler â†’ Menos afectado por outliers")
```

![](../assets/05-16.png)

```python linenums="1"
# === IMPORT PARA CLUSTERING TEST ===
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# === QUICK TEST: Â¿QuÃ© scaler funciona mejor para clustering? ===
print("QUICK TEST: Impacto en Clustering (K=4)")

clustering_results = {}
for name, X_scaled_data in X_scaled.items():
    # Aplicar K-Means con K=4
    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)  # Completar
    labels = kmeans.fit_predict(X_scaled_data)  # MÃ©todo para obtener clusters

    # Calcular silhouette score
    silhouette = silhouette_score(X_scaled_data, labels)  # MÃ©trica de calidad
    clustering_results[name] = silhouette

    print(f"   {name:>10}: Silhouette Score = {silhouette:.3f}")

# Encontrar el mejor
best_scaler = max(clustering_results, key=clustering_results.get)
best_score = clustering_results[best_scaler]

print(f"\nGANADOR: {best_scaler} (Score: {best_score:.3f})")
```

![](../assets/05-17.png)

```python linenums="1"
# === TOMAR DECISIÃ“N BASADA EN RESULTADOS ===
print("DECISIÃ“N FINAL DEL SCALER:")

# Implementar decisiÃ³n
selected_scaler_name = best_scaler  # O elige manualmente: 'MinMax', 'Standard', 'Robust'
selected_scaler = scalers[selected_scaler_name]

# Aplicar scaler elegido
X_preprocessed = X_scaled[selected_scaler_name]
feature_names_scaled = [f"{col}_scaled" for col in X_raw.columns]

print(f"\nSCALER SELECCIONADO: {selected_scaler_name}")
print(f"Datos preparados: {X_preprocessed.shape}")
print(f"Listo para PCA y Feature Selection")
```

![](../assets/05-18.png)

Breve anÃ¡lisis:

- Mejor scaler segÃºn silhouette: MinMax
- Â¿Por quÃ© crees que funcionÃ³ mejor? Debido a la poca cantidad de outliers presentes, y la manera en que manejo las variables de gÃ©nero puede haber sido superior a la de los otros scalers
- Â¿AlgÃºn scaler tuvo problemas obvios? RobustScaler a pesar de resultar de manera similar a StandardScaler, tuvo altas distinciones en el manejo de las variables de gÃ©nero (y relacionado o no a esto), y resulto con el menor Silhouette score

```python linenums="1"
from sklearn.decomposition import PCA

# === OPERACIÃ“N: DIMENSION COLLAPSE ===
print("PCA: Reduciendo dimensiones sin perder la esencia")
print("   Objetivo: De 5D â†’ 2D para visualizaciÃ³n + anÃ¡lisis de varianza")

# 1. Aplicar PCA completo para anÃ¡lisis de varianza
pca_full = PCA()
X_pca_full = pca_full.fit_transform(X_preprocessed)

# 2. ANÃLISIS DE VARIANZA EXPLICADA
explained_variance_ratio = pca_full.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)

print(f"\nğŸ“Š ANÃLISIS DE VARIANZA EXPLICADA:")
for i, (var, cum_var) in enumerate(zip(explained_variance_ratio, cumulative_variance)):
    print(f"   PC{i+1}: {var:.3f} ({var*100:.1f}%) | Acumulada: {cum_var:.3f} ({cum_var*100:.1f}%)")

# 3. VISUALIZACIÃ“N DE VARIANZA EXPLICADA
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Scree plot
axes[0].bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio,
           alpha=0.7, color='#FF6B6B')
axes[0].set_xlabel('Componentes Principales')
axes[0].set_ylabel('Varianza Explicada')
axes[0].set_title('ğŸ“Š Scree Plot - Varianza por Componente')
axes[0].set_xticks(range(1, len(explained_variance_ratio) + 1))

# Cumulative variance
axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance,
            marker='o', linewidth=2, markersize=8, color='#4ECDC4')
axes[1].axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95% threshold')
axes[1].axhline(y=0.90, color='orange', linestyle='--', alpha=0.7, label='90% threshold')
axes[1].set_xlabel('NÃºmero de Componentes')
axes[1].set_ylabel('Varianza Acumulada')
axes[1].set_title('ğŸ“ˆ Varianza Acumulada')
axes[1].legend()
axes[1].grid(True, alpha=0.3)
axes[1].set_xticks(range(1, len(cumulative_variance) + 1))

plt.tight_layout()
plt.show()

# 4. DECISIÃ“N SOBRE NÃšMERO DE COMPONENTES
print(f"\nğŸ¯ DECISIÃ“N DE COMPONENTES:")
n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1
n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1

print(f"   ğŸ“Š Para retener 90% varianza: {n_components_90} componentes")
print(f"   ğŸ“Š Para retener 95% varianza: {n_components_95} componentes")
print(f"   ğŸ¯ Para visualizaciÃ³n: 2 componentes ({cumulative_variance[1]*100:.1f}% varianza)")

# 5. APLICAR PCA CON 2 COMPONENTES PARA VISUALIZACIÃ“N
pca_2d = PCA(n_components=2, random_state=42)
X_pca_2d = pca_2d.fit_transform(X_preprocessed)

print(f"\nPCA aplicado:")
print(f"   ğŸ“Š Dimensiones: {X_preprocessed.shape} â†’ {X_pca_2d.shape}")
print(f"   ğŸ“ˆ Varianza explicada: {pca_2d.explained_variance_ratio_.sum()*100:.1f}%")

# 6. ANÃLISIS DE COMPONENTES PRINCIPALES
print(f"\nğŸ” INTERPRETACIÃ“N DE COMPONENTES:")
feature_names = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)', 'Genre_Female', 'Genre_Male']

for i, pc in enumerate(['PC1', 'PC2']):
    print(f"\n   {pc} (varianza: {pca_2d.explained_variance_ratio_[i]*100:.1f}%):")
    # Obtener los loadings (pesos de cada feature original en el componente)
    loadings = pca_2d.components_[i]
    for j, (feature, loading) in enumerate(zip(feature_names, loadings)):
        direction = "â†‘" if loading > 0 else "â†“"
        print(f"     {feature:>15}: {loading:>7.3f} {direction}")

# 7. VISUALIZACIÃ“N EN 2D
plt.figure(figsize=(12, 8))
plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], alpha=0.6, s=50, color='#96CEB4')
plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% varianza)')
plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% varianza)')
plt.title('Mall Customers en Espacio PCA 2D')
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)
plt.tight_layout()
plt.show()
```

![](../assets/05-19.png)

![](../assets/05-20.png)

![](../assets/05-21.png)

InterpretaciÃ³n del negocio:

- PC1 parece representar: Diferencia de gÃ©nero
- PC2 parece representar: Diferencia de edad y spending score
- Los clusters visibles sugieren: Segmentos separado por gÃ©nero y edad y spending score


```python linenums="1"
# === OPERACIÃ“N: FEATURE SELECTION SHOWDOWN ===
print("ğŸ¯ FEATURE SELECTION vs PCA: Â¿Seleccionar o Transformar?")
print("   ğŸ¯ Objetivo: Comparar Forward/Backward Selection vs PCA")

print(f"\nğŸ“Š FEATURE SELECTION: Forward vs Backward vs PCA")
print(f"   Dataset: {X_preprocessed.shape[0]} muestras, {X_preprocessed.shape[1]} features")

# Setup: FunciÃ³n para evaluar features en clustering
def evaluate_features_for_clustering(X, n_clusters=4):
    """EvalÃºa quÃ© tan buenas son las features para clustering usando Silhouette Score"""
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)
    return silhouette_score(X, labels)

# === IMPORTS PARA ESTIMADORES PERSONALIZADOS ===
from sklearn.base import BaseEstimator, ClassifierMixin  # Clases base necesarias

# CLASE AUXILIAR: Estimador basado en KMeans para SequentialFeatureSelector
class ClusteringEstimator(BaseEstimator, ClassifierMixin):
    """Estimador que usa KMeans y Silhouette Score para feature selection"""
    def __init__(self, n_clusters=4):
        self.n_clusters = n_clusters

    def fit(self, X, y=None):
        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)
        self.labels_ = self.kmeans_.fit_predict(X)
        return self

    def score(self, X, y=None):
        # SequentialFeatureSelector llama a score() para evaluar features
        kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X)
        return silhouette_score(X, labels)

    def predict(self, X):
        # MÃ©todo requerido por ClassifierMixin
        if hasattr(self, 'kmeans_'):
            return self.kmeans_.predict(X)
        else:
            # Si no estÃ¡ entrenado, entrenar primero
            kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)
            return kmeans.fit_predict(X)

print("âœ… Setup completado - Funciones de evaluaciÃ³n listas")
```

```python linenums="1"
# BASELINE: Todas las features
baseline_score = evaluate_features_for_clustering(X_preprocessed)
print(f"\nğŸ“Š BASELINE (todas las features): Silhouette = {baseline_score:.3f}")
print(f"   Este es el score con las {X_preprocessed.shape[1]} features originales")
print(f"   Â¿Podremos mejorar seleccionando solo las mejores 3?")
```

![](../assets/05-22.png)

```python linenums="1"
# === FORWARD SELECTION (sklearn oficial) ===
print(f"\nğŸ”„ FORWARD SELECTION (sklearn oficial):")
print(f"   Estrategia: Empezar con 0 features, agregar la mejor en cada paso")

forward_selector = SequentialFeatureSelector(
    estimator=ClusteringEstimator(n_clusters=4),  # Estimador que implementa fit() y score()
    n_features_to_select=3,
    direction='forward',  # Â¿QuÃ© direcciÃ³n para Forward?
    cv=3,
    n_jobs=-1
)

forward_selector.fit(X_preprocessed)  # MÃ©todo para entrenar
forward_mask = forward_selector.get_support()  # MÃ©todo para obtener mÃ¡scara booleana
X_forward = X_preprocessed[:, forward_mask]
forward_features = np.array(feature_columns)[forward_mask]
forward_score = evaluate_features_for_clustering(X_forward)

print(f"   Features seleccionadas: {list(forward_features)}")
print(f"   ğŸ“Š Silhouette Score: {forward_score:.3f}")
print(f"   {'âœ… Mejora!' if forward_score > baseline_score else 'âŒ Sin mejora'}")
```

```python linenums="1"
# === FORWARD SELECTION (sklearn oficial) ===
print(f"\nğŸ”„ FORWARD SELECTION (sklearn oficial):")
print(f"   Estrategia: Empezar con 0 features, agregar la mejor en cada paso")

forward_selector = SequentialFeatureSelector(
    estimator=ClusteringEstimator(n_clusters=4),  # Estimador que implementa fit() y score()
    n_features_to_select=3,
    direction='forward',  # Â¿QuÃ© direcciÃ³n para Forward?
    cv=3,
    n_jobs=-1
)

forward_selector.fit(X_preprocessed)  # MÃ©todo para entrenar
forward_mask = forward_selector.get_support()  # MÃ©todo para obtener mÃ¡scara booleana
X_forward = X_preprocessed[:, forward_mask]
forward_features = np.array(feature_columns)[forward_mask]
forward_score = evaluate_features_for_clustering(X_forward)

print(f"   Features seleccionadas: {list(forward_features)}")
print(f"   ğŸ“Š Silhouette Score: {forward_score:.3f}")
print(f"   {'âœ… Mejora!' if forward_score > baseline_score else 'âŒ Sin mejora'}")
```

![](../assets/05-23.png)

![](../assets/05-24.png)

```python linenums="1"
# === COMPARACIÃ“N FINAL DE TODOS LOS MÃ‰TODOS ===
print(f"\nğŸ“Š COMPARACIÃ“N DE MÃ‰TODOS:")
print(f"   ğŸ Baseline (todas): {baseline_score:.3f}")
print(f"   ğŸ”„ Forward Selection: {forward_score:.3f}")
print(f"   ğŸ”™ Backward Elimination: {backward_score:.3f}")

# Comparar con PCA (ya calculado anteriormente)
pca_score = evaluate_features_for_clustering(X_pca_2d)
print(f"   ğŸ“ PCA (2D): {pca_score:.3f}")

# Encontrar el mejor mÃ©todo
methods = {
    'Baseline (todas)': baseline_score,
    'Forward Selection': forward_score,
    'Backward Elimination': backward_score,
    'PCA (2D)': pca_score
}

best_method = max(methods, key=methods.get)
best_score = methods[best_method]

print(f"\nğŸ† GANADOR: {best_method} con score = {best_score:.3f}")

# AnÃ¡lisis de diferencias
print(f"\nğŸ” ANÃLISIS:")
for method, score in sorted(methods.items(), key=lambda x: x[1], reverse=True):
    improvement = ((score - baseline_score) / baseline_score) * 100
    print(f"   {method}: {score:.3f} ({improvement:+.1f}% vs baseline)")
```

![](../assets/05-25.png)

```python linenums="1"
# === VISUALIZACIÃ“N DE COMPARACIÃ“N ===
methods_names = ['Baseline', 'Forward', 'Backward', 'PCA 2D']
scores_values = [baseline_score, forward_score, backward_score, pca_score]
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']

plt.figure(figsize=(12, 6))
bars = plt.bar(methods_names, scores_values, color=colors, alpha=0.7)
plt.ylabel('Silhouette Score')
plt.title('ComparaciÃ³n de MÃ©todos de Feature Selection')
plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Threshold Aceptable (0.5)')
plt.axhline(y=0.7, color='green', linestyle='--', alpha=0.5, label='Threshold Muy Bueno (0.7)')
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

# AÃ±adir valores en las barras
for bar, score in zip(bars, scores_values):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()
```

![](../assets/05-26.png)

```python linenums="1"
# === ANÃLISIS DE RESULTADOS ===
print(f"\nğŸ¯ ANÃLISIS DE RESULTADOS:")

# Comparar features seleccionadas
print(f"\nğŸ” FEATURES SELECCIONADAS POR CADA MÃ‰TODO:")
print(f"   ğŸ”„ Forward Selection: {list(forward_features)}")
print(f"   ğŸ”™ Backward Elimination: {list(backward_features)}")

# AnÃ¡lisis de coincidencias
forward_set = set(forward_features)
backward_set = set(backward_features)

common_forward_backward = forward_set & backward_set

print(f"\nğŸ¤ COINCIDENCIAS:")
print(f"   Forward âˆ© Backward: {list(common_forward_backward)}")
print(f"   Â¿Seleccionaron las mismas features? {'SÃ­' if forward_set == backward_set else 'No'}")
```

![](../assets/05-27.png)

Preguntas de anÃ¡lisis:

- MÃ©todo con mejor score: PCA
- Â¿Forward y Backward seleccionaron exactamente las mismas features? SÃ­
- Â¿PCA con 2 componentes es competitivo? SÃ­, superando tanto la baseline como a forward y backward selection
- Â¿AlgÃºn mÃ©todo superÃ³ el threshold de 0.5? PCA, forward y backward
- Â¿La reducciÃ³n de dimensionalidad mejorÃ³ el clustering? SÃ­

```python linenums="1"
# === DECISIÃ“N PARA EL ANÃLISIS FINAL ===
print(f"\nğŸ¢ DECISIÃ“N PARA EL ANÃLISIS:")

# Decidir mÃ©todo basado en resultados
if best_score == pca_score:
    selected_method = "PCA"
    selected_data = X_pca_2d
    print(f"   ğŸ¯ SELECCIONADO: PCA (2D) - Score: {pca_score:.3f}")
    print(f"   âœ… RAZÃ“N: Mejor balance entre reducciÃ³n dimensional y performance")
elif best_score == forward_score:
    selected_method = "Forward Selection"
    selected_data = X_forward
    print(f"   ğŸ¯ SELECCIONADO: Forward Selection - Score: {forward_score:.3f}")
    print(f"   âœ… RAZÃ“N: Mejor score con features interpretables")
elif best_score == backward_score:
    selected_method = "Backward Elimination"
    selected_data = X_backward
    print(f"   ğŸ¯ SELECCIONADO: Backward Elimination - Score: {backward_score:.3f}")
    print(f"   âœ… RAZÃ“N: Mejor score eliminando features redundantes")
else:
    # Fallback to baseline if needed
    selected_method = "Baseline (todas las features)"
    selected_data = X_preprocessed
    print(f"   ğŸ¯ SELECCIONADO: Baseline - Score: {baseline_score:.3f}")
    print(f"   âœ… RAZÃ“N: NingÃºn mÃ©todo de reducciÃ³n mejorÃ³ el clustering")

# Guardar para clustering final
X_final_for_clustering = selected_data
final_method_name = selected_method

print(f"\nğŸ“Š PREPARADO PARA CLUSTERING:")
print(f"   MÃ©todo: {final_method_name}")
print(f"   Dimensiones: {X_final_for_clustering.shape}")
print(f"   Silhouette Score: {best_score:.3f}")
```

![](../assets/05-28.png)

### 4. Modelado y clustering

```python linenums="1"
# === OPERACIÃ“N: CUSTOMER SEGMENTATION DISCOVERY ===
print("K-MEANS CLUSTERING: Descubriendo segmentos de clientes")
print(f"   Dataset: {X_final_for_clustering.shape} usando mÃ©todo '{final_method_name}'")

# 1. BÃšSQUEDA DEL K Ã“PTIMO - Elbow Method + Silhouette
print(f"\nğŸ“ˆ BÃšSQUEDA DEL K Ã“PTIMO:")

k_range = range(2, 9)
inertias = []
silhouette_scores = []

for k in k_range:
    # Aplicar K-Means
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_final_for_clustering)

    # Calcular mÃ©tricas
    inertias.append(kmeans.inertia_)
    sil_score = silhouette_score(X_final_for_clustering, labels)
    silhouette_scores.append(sil_score)

    print(f"   K={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={sil_score:.3f}")

# 2. VISUALIZACIÃ“N ELBOW METHOD + SILHOUETTE
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Elbow Method
axes[0].plot(k_range, inertias, marker='o', linewidth=2, markersize=8, color='#FF6B6B')
axes[0].set_xlabel('NÃºmero de Clusters (K)')
axes[0].set_ylabel('Inertia (WCSS)')
axes[0].set_title('ğŸ“ˆ Elbow Method')
axes[0].grid(True, alpha=0.3)
axes[0].set_xticks(k_range)

# Silhouette Scores
axes[1].plot(k_range, silhouette_scores, marker='s', linewidth=2, markersize=8, color='#4ECDC4')
axes[1].axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Aceptable (0.5)')
axes[1].axhline(y=0.7, color='green', linestyle='--', alpha=0.7, label='Muy Bueno (0.7)')
axes[1].set_xlabel('NÃºmero de Clusters (K)')
axes[1].set_ylabel('Silhouette Score')
axes[1].set_title('ğŸ“Š Silhouette Analysis')
axes[1].legend()
axes[1].grid(True, alpha=0.3)
axes[1].set_xticks(k_range)

plt.tight_layout()
plt.show()

# 3. ANÃLISIS DEL ELBOW METHOD
print(f"\nğŸ§  ELBOW METHOD - DEEP DIVE ANALYSIS:")
print(f"\nğŸ“‰ **Â¿QuÃ© es exactamente 'el codo'?**")
print(f"   - **MatemÃ¡ticamente:** Punto donde la segunda derivada de WCSS vs K cambia mÃ¡s dramÃ¡ticamente")
print(f"   - **Visualmente:** Donde la curva pasa de 'caÃ­da empinada' a 'caÃ­da suave'")
print(f"   - **Conceptualmente:** Balance entre simplicidad (menos clusters) y precisiÃ³n (menor error)")

# Calcular diferencias para encontrar el codo
differences = np.diff(inertias)
second_differences = np.diff(differences)
elbow_candidate = k_range[np.argmin(second_differences) + 2]  # +2 por los dos diff()

print(f"\nğŸ“Š **AnÃ¡lisis cuantitativo del codo:**")
for i, k in enumerate(k_range[:-2]):
    print(f"   K={k}: Î” Inertia={differences[i]:.2f}, Î”Â²={second_differences[i]:.2f}")

print(f"\nğŸ¯ **Candidato por Elbow Method:** K={elbow_candidate}")

# 4. DECISIÃ“N FINAL DE K
best_k_silhouette = k_range[np.argmax(silhouette_scores)]
print(f"ğŸ¯ **Candidato por Silhouette:** K={best_k_silhouette} (score={max(silhouette_scores):.3f})")

print(f"\nğŸ¤ **DECISIÃ“N FINAL:**")
if elbow_candidate == best_k_silhouette:
    optimal_k = elbow_candidate
    print(f"   Ambos mÃ©todos coinciden: K = {optimal_k}")
else:
    print(f"   âš–ï¸  Elbow sugiere K={elbow_candidate}, Silhouette sugiere K={best_k_silhouette}")
    print(f"   ğŸ’¼ Considerando el contexto de negocio (3-5 segmentos esperados)...")
    # Elegir basado en contexto de negocio y calidad
    if 3 <= best_k_silhouette <= 5 and max(silhouette_scores) > 0.4:
        optimal_k = best_k_silhouette
        print(f"   Elegimos K = {optimal_k} (mejor silhouette + contexto negocio)")
    else:
        optimal_k = elbow_candidate if 3 <= elbow_candidate <= 5 else 4
        print(f"   Elegimos K = {optimal_k} (balance elbow + contexto negocio)")

# 5. MODELO FINAL CON K Ã“PTIMO
print(f"\nğŸ¯ ENTRENANDO MODELO FINAL CON K={optimal_k}")

final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=20)
final_labels = final_kmeans.fit_predict(X_final_for_clustering)
final_silhouette = silhouette_score(X_final_for_clustering, final_labels)

print(f"Modelo entrenado:")
print(f"   ğŸ“Š Silhouette Score: {final_silhouette:.3f}")
print(f"   ğŸ¯ Clusters encontrados: {optimal_k}")
print(f"   ğŸ“ˆ Inertia final: {final_kmeans.inertia_:.2f}")

# 6. DISTRIBUCIÃ“N DE CLIENTES POR CLUSTER
cluster_counts = pd.Series(final_labels).value_counts().sort_index()
print(f"\nğŸ‘¥ DISTRIBUCIÃ“N DE CLIENTES:")
for cluster_id, count in cluster_counts.items():
    percentage = (count / len(final_labels)) * 100
    print(f"   Cluster {cluster_id}: {count:,} clientes ({percentage:.1f}%)")

# 7. AGREGAR CLUSTERS AL DATAFRAME ORIGINAL
df_customers['cluster'] = final_labels
df_customers['cluster_name'] = df_customers['cluster'].map({
    i: f"Cluster_{i}" for i in range(optimal_k)
})

print(f"\nClusters asignados al dataset original")
```

![](../assets/05-29.png)

![](../assets/05-30.png)

![](../assets/05-31.png)

### 5. EvaluaciÃ³n y resumen

```python linenums="1"
# === OPERACIÃ“N: INTELLIGENCE REPORT ===
print("ANALISIS DE SEGMENTOS DE CLIENTES - REPORTE EJECUTIVO")

# 1. PERFILES DE CLUSTERS
print(f"\nPERFILES DETALLADOS POR CLUSTER:")

for cluster_id in sorted(df_customers['cluster'].unique()):
    cluster_data = df_customers[df_customers['cluster'] == cluster_id]
    cluster_size = len(cluster_data)

    print(f"\n**CLUSTER {cluster_id}** ({cluster_size} clientes, {cluster_size/len(df_customers)*100:.1f}%)")

    # EstadÃ­sticas usando las columnas CORRECTAS del Mall Customer Dataset
    avg_age = cluster_data['Age'].mean()
    avg_income = cluster_data['Annual Income (k$)'].mean()
    avg_spending = cluster_data['Spending Score (1-100)'].mean()

    # DistribuciÃ³n por gÃ©nero
    genre_counts = cluster_data['Genre'].value_counts()

    print(f"   **Perfil DemogrÃ¡fico:**")
    print(f"      Edad promedio: {avg_age:.1f} aÃ±os")
    print(f"      DistribuciÃ³n gÃ©nero: {dict(genre_counts)}")

    print(f"   **Perfil Financiero:**")
    print(f"      Ingreso anual: ${avg_income:.1f}k")
    print(f"      Spending Score: {avg_spending:.1f}/100")

    # Comparar con ground truth si estÃ¡ disponible
    if 'true_segment' in df_customers.columns:
        true_segments_in_cluster = cluster_data['true_segment'].value_counts()
        dominant_segment = true_segments_in_cluster.index[0]
        purity = true_segments_in_cluster.iloc[0] / cluster_size
        print(f"   ğŸ¯ **Ground Truth:** {dominant_segment} ({purity*100:.1f}% purity)")

# 2. MATRIZ DE CONFUSIÃ“N CON GROUND TRUTH
if 'true_segment' in df_customers.columns:
    print(f"\nğŸ¯ VALIDACIÃ“N CON GROUND TRUTH:")
    confusion_matrix = pd.crosstab(df_customers['true_segment'], df_customers['cluster'],
                                  margins=True, margins_name="Total")
    print(confusion_matrix)

    # Calcular pureza de clusters
    cluster_purities = []
    for cluster_id in sorted(df_customers['cluster'].unique()):
        cluster_data = df_customers[df_customers['cluster'] == cluster_id]
        dominant_true_segment = cluster_data['true_segment'].mode().iloc[0]
        purity = (cluster_data['true_segment'] == dominant_true_segment).mean()
        cluster_purities.append(purity)

    average_purity = np.mean(cluster_purities)
    print(f"\nğŸ“Š Pureza promedio de clusters: {average_purity:.3f}")

# 3. VISUALIZACIÃ“N DE CLUSTERS
if final_method_name == 'PCA':  # Si usamos PCA, podemos visualizar en 2D
    plt.figure(figsize=(15, 10))

    # Subplot 1: Clusters encontrados
    plt.subplot(2, 2, 1)
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
    for cluster_id in sorted(df_customers['cluster'].unique()):
        cluster_mask = final_labels == cluster_id
        plt.scatter(X_pca_2d[cluster_mask, 0], X_pca_2d[cluster_mask, 1],
                   c=colors[cluster_id % len(colors)], label=f'Cluster {cluster_id}',
                   alpha=0.7, s=50)

    # Plotear centroides
    if final_method_name == 'PCA':
        centroids_pca = final_kmeans.cluster_centers_
        plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1],
                   c='red', marker='X', s=200, linewidths=3, label='Centroides')

    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.title('Clusters Descubiertos (PCA 2D)')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Subplot 2: Ground truth (si disponible)
    if 'true_segment' in df_customers.columns:
        plt.subplot(2, 2, 2)
        true_segment_colors = {'VIP': '#FF6B6B', 'Regular': '#4ECDC4',
                              'Occasional': '#45B7D1', 'At_Risk': '#96CEB4'}
        for segment, color in true_segment_colors.items():
            segment_mask = df_customers['true_segment'] == segment
            segment_indices = df_customers[segment_mask].index
            plt.scatter(X_pca_2d[segment_indices, 0], X_pca_2d[segment_indices, 1],
                       c=color, label=segment, alpha=0.7, s=50)

        plt.xlabel('PC1')
        plt.ylabel('PC2')
        plt.title('Ground Truth Segments')
        plt.legend()
        plt.grid(True, alpha=0.3)

    # Subplot 3: Feature distribution by cluster
    plt.subplot(2, 2, 3)
    # Usar las columnas correctas del Mall Customer Dataset
    cluster_means = df_customers.groupby('cluster')[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].mean()
    cluster_means.plot(kind='bar', ax=plt.gca(), color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
    plt.title('Perfil Promedio por Cluster')
    plt.ylabel('Valor Promedio')
    plt.legend(title='CaracterÃ­sticas', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.xticks(rotation=0)

    # Subplot 4: Cluster sizes
    plt.subplot(2, 2, 4)
    cluster_sizes = df_customers['cluster'].value_counts().sort_index()
    colors_subset = [colors[i] for i in cluster_sizes.index]
    bars = plt.bar(cluster_sizes.index, cluster_sizes.values, color=colors_subset, alpha=0.7)
    plt.xlabel('Cluster ID')
    plt.ylabel('NÃºmero de Clientes')
    plt.title('DistribuciÃ³n de Clientes por Cluster')

    # AÃ±adir etiquetas en las barras
    for bar, size in zip(bars, cluster_sizes.values):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,
                f'{size}\n({size/len(df_customers)*100:.1f}%)',
                ha='center', va='bottom')

plt.tight_layout()
plt.show()
```

![](../assets/05-32.png)

![](../assets/05-33.png)

```python linenums="1"
# === ANÃLISIS SILHOUETTE POR CLUSTER ===
print(f"\nğŸ“Š ANÃLISIS SILHOUETTE DETALLADO:")

from sklearn.metrics import silhouette_samples  # FunciÃ³n para silhouette por muestra individual

# Calcular silhouette score por muestra
sample_silhouette_values = silhouette_samples(X_final_for_clustering, final_labels)

# EstadÃ­sticas por cluster
print(f"   ğŸ¯ Silhouette Score General: {final_silhouette:.3f}")
for cluster_id in sorted(df_customers['cluster'].unique()):
    cluster_silhouette_values = sample_silhouette_values[final_labels == cluster_id]
    cluster_avg_silhouette = cluster_silhouette_values.mean()
    cluster_min_silhouette = cluster_silhouette_values.min()

    print(f"   Cluster {cluster_id}: Î¼={cluster_avg_silhouette:.3f}, "
          f"min={cluster_min_silhouette:.3f}, "
          f"samples={len(cluster_silhouette_values)}")
```

![](../assets/05-34.png)

```python linenums="1"
# === DETECCIÃ“N DE OUTLIERS EN CLUSTERING ===
print(f"\nğŸš¨ DETECCIÃ“N DE OUTLIERS EN CLUSTERING:")
outlier_threshold = 0.0  # Silhouette negativo = mal asignado

for cluster_id in sorted(df_customers['cluster'].unique()):
    cluster_mask = final_labels == cluster_id
    cluster_silhouette = sample_silhouette_values[cluster_mask]
    outliers = np.sum(cluster_silhouette < outlier_threshold)

    if outliers > 0:
        print(f"   âš ï¸  Cluster {cluster_id}: {outliers} posibles outliers (silhouette < 0)")
    else:
        print(f"   âœ… Cluster {cluster_id}: Sin outliers detectados")
```

![](../assets/05-35.png)

```python linenums="1"
# === ANÃLISIS DE PERFILES POR CLUSTER ===
print(f"\nANALISIS DE SEGMENTOS DE CLIENTES - REPORTE EJECUTIVO")
print(f"\nPERFILES DETALLADOS POR CLUSTER:")

# AnÃ¡lisis por cluster usando las columnas REALES del dataset
for cluster_id in sorted(df_customers['cluster'].unique()):
    cluster_data = df_customers[df_customers['cluster'] == cluster_id]
    cluster_size = len(cluster_data)
    cluster_pct = (cluster_size / len(df_customers)) * 100

    # EstadÃ­sticas usando las columnas CORRECTAS del Mall Customer Dataset
    avg_age = cluster_data['Age'].mean()
    avg_income = cluster_data['Annual Income (k$)'].mean()
    avg_spending = cluster_data['Spending Score (1-100)'].mean()

    # DistribuciÃ³n por gÃ©nero
    genre_counts = cluster_data['Genre'].value_counts()

    print(f"\nğŸ·ï¸  **CLUSTER {cluster_id}** ({cluster_size} clientes, {cluster_pct:.1f}%)")
    print(f"   ğŸ“Š **Perfil DemogrÃ¡fico:**")
    print(f"      ğŸ‘¤ Edad promedio: {avg_age:.1f} aÃ±os")
    print(f"      ğŸ‘¥ DistribuciÃ³n gÃ©nero: {dict(genre_counts)}")

    print(f"   ğŸ’° **Perfil Financiero:**")
    print(f"      ğŸ’µ Ingreso anual: ${avg_income:.1f}k")
    print(f"      ğŸ›ï¸  Spending Score: {avg_spending:.1f}/100")
```

![](../assets/05-36.png)

### Preguntas posteriores

- Â¿QuÃ© fase fue mÃ¡s desafiante y por quÃ©? _



- Â¿CÃ³mo el entendimiento del negocio influyÃ³ en tus decisiones tÃ©cnicas? _

Al no llegar a un acuerdo del K Ã³ptimo con el Elbow Method y Silhouette, se tomÃ³ una
decisiÃ³n en base a la solicitud del negocio de manter los segmentos entre 3 y 5, y no
usÃ³ simplemente el Elbow Method o el Silhouette sino un promedio de los dos, cuyo 
resultado entraba en el rango anterior.

- Â¿QuÃ© scaler funcionÃ³ mejor y por quÃ©? _



- Â¿PCA o Feature Selection fue mÃ¡s efectivo para tu caso? _

En este caso, PCA fue mÃ¡s efectivo que ambos Feature Selection con un mÃ¡rgen de
+0.113 en base al Silhouette Score.

- Â¿CÃ³mo balanceaste interpretabilidad vs performance? _



- Â¿El Elbow Method y Silhouette coincidieron en el K Ã³ptimo?

No, el Elbow Method proponÃ­a un nÃºmero de mayor de clusters, mientras que en base
al Silhouette Score se proponÃ­a sÃ³lo un par. Tomando en cuenta el contexto de negocio,
se llegÃ³ al resultado intermedio de 4 clusters.

- Â¿Los clusters encontrados coinciden con la intuiciÃ³n de negocio? _



- Â¿QuÃ© harÃ­as diferente si fueras a repetir el anÃ¡lisis? _



- Â¿CÃ³mo presentarÃ­as estos resultados en un contexto empresarial? _



- Â¿QuÃ© valor aportan estas segmentaciones? _



- Â¿QuÃ© limitaciones tiene este anÃ¡lisis? _



## Evidencias
- [Link al Colab](https://colab.research.google.com/drive/1CF5oHvPmK4o1lfW5oue_HXQhEuPSry79?usp=sharing)

## ReflexiÃ³n
- QuÃ© aprendiste, quÃ© mejorarÃ­as, prÃ³ximos pasos

## Referencias
- Fuentes consultadas con enlaces relativos cuando corresponda