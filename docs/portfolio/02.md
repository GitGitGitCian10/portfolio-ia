---
title: "Entrada 02 ‚Äî Feature Engineering simple + Modelo base"
date: 2025-09-21
---

# Entrada 02 ‚Äî Feature Engineering simple + Modelo base

## Contexto
En esta actividad realizaremos feature engineering con el dataset
del Titanic para mejorar la evaluaci√≥n de un modelo de regresi√≥n
log√≠stica que luego compararemos con un modelo base para validar su
efectividad.

## Objetivos
- Comprender la utilidad del feature engineering y aplicarlo en un
ejemplo visto anteriormente.
- Entender la funci√≥n de la regresi√≥n log√≠stica y aplicarlo para
predecir la supervivencia de los pasajeros del Titanic.
- Utilizar un modelo arbitrario dummy como baseline para validar
el funcionamiento del modelo entrenado.
- Utilizar el reporte de clasificaci√≥n y matriz de confusi√≥n para
tener un mejor an√°lisis de lo resultados y efectividad del modelo.

## Actividades (con tiempos estimados)
- Investigaci√≥n de Scikit-learn ‚Äî 10 min
- Preprocesamiento y features ‚Äî 15 min
- Modelo base y baseline ‚Äî 20 min

## Desarrollo
### 1. Investigaci√≥n de Scikit-learn

#### LogisticRegression:

- La regresi√≥n log√≠stica predice valores discretos, sirve para resolver
problemas de clasificaci√≥n binaria, como decidir si un mail es spam o no.
- Parametros importantes:
    - penalty: define el tipo de norma usada para regularizaci√≥n,
    se utiliza para evitar el overfitting.
    - C: inversa de la fuerza de regularizaci√≥n.
    - solver: algoritmo usado para optimizaci√≥n.
    - max_iter: m√°ximo n√∫mero de iteraciones para converger.
    - fit_intercept: determina si el modelo calcula el bias.
- Se utiliza liblinear solver uando se quiere resolver problemas de 
clasificaci√≥n en datasets peque√±os.

#### DummyClassifier:

- DummyClassifier realiza preiddciones ignorando las features de input,
sirve para verificar que un modelo que deber√≠a funcionar realmente
prediga mejor que una predicci√≥n aleatorio o sin fundamentos.
- Estrat√©gias:
    - most_frequent: predice la clase m√°s frecuente del conjunto y.
    - stratified: predice aleatoriamente con pesos basados en el
    conjunto y.
    - prior: idem a most_frequent con ligeras diferencias.
    - uniform: predice clases uniformemente aleatorias.
    - constant: siempre predice la clase provista por el usuario.
- Es importante tener un baseline para validar que el modelo que
estemos entrenando prediga de manera efectiva y no sea peor a una
predicci√≥n arbitraria o aleatoria.

#### train_test_split:

- Stratify permite que la proporci√≥n de clases pasada al parametro
se mantenga en los conjuntos de train y test.
- Para asegurar que los conjuntos de train y test sean los mismos
cada ejecuci√≥n permitiendo obtener resultados fiables.
- De manera predeterminada este valor es 0.25 si tampoco se setea
el tama√±o del train set, por tanto tomo ese valor como recomendable.

#### M√©tricas de evaluaci√≥n:

- M√©tricas:
    - Precision: TP / (TP + FP).
    - Recall: TP / (TP + FN).
    - F1-score: media harm√≥nico de precision y recall.
    - Support: cantidad de caso de cada clase.
- En la posici√≥n C<sub>i,j</sub> de la matriz representa los casos
donde la clase era i y el modelo predijo j.
- Est√° bien usar accuracy en vez de otras m√©tricas cuando el dataset
est√° balanceado en sus clases.


### 2. Preprocesamiento y features

Con el mismo dataset de la entrada anterior, utilizaremos Feature Engineering
para crear features nuevas (con base en las existentes) al dataset que 
consideremos puedan ser de utilidad para un posterior an√°lisis.

```python linenums="1"
df = train.copy()

# üö´ PASO 1: Manejar valores faltantes (imputaci√≥n)
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])  # Valor m√°s com√∫n
df['Fare'] = df['Fare'].fillna(df['Fare'].median())              # Mediana
df['Age'] = df['Age'].fillna(df.groupby(['Sex','Pclass'])['Age'].transform('median'))
```

En este paso, se imputan las columnas pertinentes que contienen valores 
faltantes con diferentes m√©todos.

Mediana para tanto la tarifa como la edad, ya que no genera valores inexistentes
y no se ve tan alterada por casos at√≠picos. Como embarked no tiene valores
n√∫mericos, utiliza el valor m√°s frecuente.

```python linenums="1"
# üÜï PASO 2: Crear nuevas features √∫tiles
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
df['IsAlone'] = (df['FamilySize'] == 1).astype(int)

df['Title'] = df['Name'].str.extract(',\s*([^\.]+)\.')
rare_titles = df['Title'].value_counts()[df['Title'].value_counts() < 10].index
df['Title'] = df['Title'].replace(rare_titles, 'Rare')
```

A continuaci√≥n, genera features que podrian ser de utilidad para el an√°lisis,
como el tama√±o de familia a bordo (bas√°ndose en la cantidad de hermanos, c√≥nyuges,
padres e hijos), opuesto a lo anterior si viaja s√≥lo (familia = 1) y dividiendo 
el nombre junto con el t√≠tulo en nombre y t√≠tulo.

```python linenums="1"
# üîÑ PASO 3: Preparar datos para el modelo
features = ['Pclass','Sex','Age','Fare','Embarked','FamilySize','IsAlone','Title','SibSp','Parch']
X = pd.get_dummies(df[features], drop_first=True)
y = df['Survived']

X.shape, y.shape
```

![](../assets/02-01.png)

Por √∫ltimo, separa el dataset en las features de input y el resultado esperado
(X e y), adem√°s de aplicar una modificaci√≥n a X transformando las columnas
categ√≥ricas (por ejemplo, sexo siendo male o female) en sexo_male teniendo
valores true o false.

### 3. Modelo base y baseline

```python linenums="1"
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

dummy = DummyClassifier(strategy='most_frequent', random_state=42)
dummy.fit(X_train, y_train)
baseline_pred = dummy.predict(X_test)

lr = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)
lr.fit(X_train, y_train)
pred = lr.predict(X_test)

print('Baseline acc:', accuracy_score(y_test, baseline_pred))
print('LogReg acc  :', accuracy_score(y_test, pred))

print('\nClassification report (LogReg):')
print(classification_report(y_test, pred))

print('\nConfusion matrix (LogReg):')
print(confusion_matrix(y_test, pred))
```

![](../assets/02-02.png)

### Preguntas posteriores
- Matriz de confusi√≥n: ¬øEn qu√© casos se equivoca m√°s el modelo: cuando
predice que una persona sobrevivi√≥ y no lo hizo, o al rev√©s?

Se equivoca m√°s cuando predice que muri√≥ y est√° vivo (FN).

- Clases atendidas: ¬øEl modelo acierta m√°s con los que sobrevivieron
o con los que no sobrevivieron?

Acierta m√°s con los que no sobrevivieron, teniendo 98 de 110 predicciones
correctas frente a 48 de 69.

- Comparaci√≥n con baseline: ¬øLa Regresi√≥n Log√≠stica obtiene m√°s aciertos
que el modelo que siempre predice la clase m√°s com√∫n?

La Regresi√≥n Log√≠stica acierta m√°s que el DummyClassifier dado que consigui√≥
81% de aciertos frente a los 61% del dummy.

- Errores m√°s importantes: ¬øCu√°l de los dos tipos de error cre√©s que es
m√°s grave para este problema?

Considero que es peor predecir que alguien sobrevivi√≥
cuando no lo hizo en vez de predecir que muri√≥ estando vivo, pensando
en las personas cercanas al afectado y temas de propiedad legal.

- Observaciones generales: Mirando las gr√°ficas y n√∫meros, ¬øqu√© patrones
encontraste sobre la supervivencia?

No aplica a esta entrada, refi√©rase a [Titanic: Exploraci√≥n de datos y descubrimiento de patrones de supervivencia](01.md).

- Mejoras simples: ¬øQu√© nueva columna (feature) se te ocurre que podr√≠a
ayudar a que el modelo acierte m√°s?

Una columna que bas√°ndose en el t√≠tulo, sexo y SibSp determine si una persona
estaba casada con otra en el barco u otra columna que divida las edades en
diferentes rangos etarios de importancia (beb√©, ni√±o, adulto, anciano, etc.).


## Evidencias
- [Link al colab](https://colab.research.google.com/drive/18zOq-MXkOFXdz7T_GNKs0AvHLyjihmyX?usp=sharing)

## Reflexi√≥n
Considero que el feature engineering puede ser muy √∫til cuando se puede inferir
features que podr√≠an ayudar a las predicciones de los modelos o incluso
simplemente para facilitar la comprensi√≥n del dataset. Adem√°s la comparaci√≥n con
varios baselines parece ser algo bastante importante para implementar en futuras
iteraciones.

Finalmente, considero que fue una buena progresi√≥n pasar del an√°lisis del dataset
del Titanic a un modelo simple prediciendo la supervivencia de los pasajeros.

## Referencias

- [Mastering Logistic Regression with Scikit-Learn: A Complete Guide](https://www.digitalocean.com/community/tutorials/logistic-regression-with-scikit-learn)